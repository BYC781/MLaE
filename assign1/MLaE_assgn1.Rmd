---
title: "MLaE: Assignment #1"
author: "Boyu, Chen R11323006"
date: "2023-03-19"
output: pdf_document
---



# Q1
The square of Euclidean norm represented by matrix form is 

\begin{align*}
&\| \beta \|^2_2 = \sum_j |\beta_j|^2 = \sum_i \beta_j^2 = \beta^T \beta\\
&\lambda \| \beta \|^2_2 = \lambda \sum_i |\beta_j|^2 = \lambda \sum_i \beta_j^2 = \beta^T \lambda I \beta
\end{align*}

Let $Q$ denote the objective function, $Q = (Y-X\beta)^T(Y-X\beta) + \lambda\| \beta \|^2_2$

\begin{align*}
\hat\beta &= \mathop{\arg\min}_{\beta}Q \\
&= \mathop{\arg\min}_{\beta} (Y-X\beta)^T(Y-X\beta) + \beta^T \lambda  I\beta\\ 
&= \mathop{\arg\min}_{\beta} Y^TY - 2Y^TX\beta+\beta^TX^TX\beta+ \beta^T \lambda I \beta
\end{align*}

By the First-order condition w.r.t.$\beta$, we have

\begin{align*}
F.O.C. \\
\\
&\frac{\partial Q}{\partial \beta} = -2X^TY+2X^TX\hat\beta+2\lambda I \hat\beta =0\\
& \Rightarrow -X^TY + (X^TX+I)\hat\beta = 0 \\
& \Rightarrow \hat\beta_{Ridge} = (X^TX + \lambda I)^{-1}X^TY
\end{align*}


# Q2
```{r setup, include=FALSE}
library(glmnet)
library(dplyr)
source("Function.R")
```

### Set Parameters
```{r parameters_iniitialize}
beta_vector <- rep(1, 21)
beta <- setNames(beta_vector, paste0("b", 0:20))
n <- 500; p <- 50; tra.idx <- 1:400; R <- 20
```

### The program for a. and b.
```{r main1}
# main1: The variable naming rule is "beta_method_dgp_model"
set.seed(1234)
container <- matrix(nrow=R, ncol=24)
params <- expand.grid(b_hat=c(1, 21), 
                      met=c("OLS", "Ridge", "LASSO"), dt=1:2, md=1:2)
colnames(container) <- apply(params, 1, paste0, collapse="_")
container[] <- mapply(function(b_hat, met, dt, md) {
    sapply(1:R, function(...) 
        draw_and_get(method=met, beta=b_hat, model=md, dgp=dt))
}, params$b_hat,params$met,params$dt,params$md)
```

### The program for c.
```{r main2}
# main2: The variable naming rule is "method_dgp_model"
set.seed(1234)
params2 <- expand.grid(met=c("OLS", "Ridge", "LASSO"), dt=1:2, md=1:2)
matrix_list <- lapply(1:12, function(x) matrix(nrow = 100, ncol = R))
matrix_list <- lapply(seq_along(matrix_list), function(i) {
    sapply(1:R, function(r){
        draw_and_pred_minus_test(met=params2[i,"met"], 
                                 model=params2[i,"md"], 
                                 dgp=params2[i,"dt"])
    })
})
names(matrix_list) <- apply(params2, 1, paste0, collapse="_")
```

### Result for a.
In this sub-question, the naming rule is "beta_method_dgp_model", for example, `1_OLS_1_1` stands for estimate $\beta_1$ by "OLS" and "DGP1" and "Model 1".
```{r}
##a
mean.for.b1 <- apply(container[,seq(1,24,2)], 2, mean)
sd.for.b1 <- apply(container[,seq(1,24,2)], 2, var) %>% sqrt()
rmses.for.b1 <- apply(container[,seq(1,24,2)], 2, 
                      function(col) sqrt((1/length(col)) * sum((col - 1)^2)))
a_result <- cbind(mean.for.b1, sd.for.b1, rmses.for.b1)
a_result ## row names are "beta_method_dgp_model"
```
### Result for b.
In this sub-question, the naming rule is "beta_method_dgp_model", for example, `21_OLS_1_1` stands for estimate $\beta_{21}$ by "OLS" and "DGP1" and "Model 1".
```{r}
##b
mean.for.b21 <- apply(container[,seq(2,24,2)], 2, mean)
sd.for.b21 <- apply(container[,seq(2,24,2)], 2, var) %>% sqrt()
rmses.for.b21 <- apply(container[,seq(2,24,2)], 2, 
                       function(col) sqrt((1/length(col)) * sum((col - 0)^2)))
b_result <- cbind(mean.for.b21, sd.for.b21, rmses.for.b21)
b_result ## row names are "beta_method_dgp_model"
```
### Result for c.
In this sub-question, the naming rule is "method_dgp_model", for example, `OLS_1_1` stands for the MSPE calculated by "OLS" and "DGP1" and "Model 1".
```{r}
##c
MSPE_method_dgp_model <- numeric(12)
for (i in 1:12){
    sum <- 0
    for (col in 1:R){
        sum <- sum + 0.01*t(matrix_list[[i]][,col]) %*% matrix_list[[i]][,col]
    }
    result <- R^-1 * sum
    MSPE_method_dgp_model[i] <- result
}
names(MSPE_method_dgp_model) <- apply(params2, 1, paste0, collapse="_")
MSPE_method_dgp_model ## "method_dgp_model"
```


# Appendix: My Functions
```{r function}
get_coef <- function(method, beta, model, dgp){
    if (model == 1) X <- X[,1:26]
    if (dgp == 1) y <- y_dgp1 else y <- y_dgp2
    if(method == "OLS"){
        return(lm(y[tra.idx,] ~ X[tra.idx,])$coefficients[beta+1])
    }
    else if (method == "Ridge"){
        ridge_model <- cv.glmnet(X[tra.idx, ],y[tra.idx, ],
                                 alpha = 0,
                                 nfolds =10)
        ridge_best <- glmnet(X, y, alpha = 0,lambda = ridge_model$lambda.min)
        return(coef(ridge_best)[beta+1])
    }
    else if(method == "LASSO"){
        lasso_model <- cv.glmnet(X[tra.idx, ],y[tra.idx, ],
                                 alpha = 1,
                                 nfolds =10)
        lasso_best <- glmnet(X, y, alpha = 1,lambda = lasso_model$lambda.min)
        return(coef(lasso_best)[beta+1])
    }
}

get_pred_minus_test <- function(method, model, dgp){
    if (model == 1) X <- X[,1:26]
    if (dgp == 1) y <- y_dgp1 else y <- y_dgp2
    if (method == "OLS"){
        train_df <- cbind(data.frame(Y=y[tra.idx,]), data.frame(X[tra.idx,]))
        new <- data.frame(X[-tra.idx,])
        my_lm <- lm(Y~., data=train_df)
        return(predict(my_lm, new) - y[-tra.idx,])
        
    }else if (method == "Ridge"){
        ridge_model <- cv.glmnet(X[tra.idx, ],y[tra.idx, ],
                           alpha = 0,
                           nfolds =10)
        
        ridge_best <- glmnet(X[tra.idx,], y[tra.idx,], alpha = 0,
                             lambda = ridge_model$lambda.min)
        
        return(predict.glmnet(ridge_best, X[-tra.idx,]) - y[-tra.idx,])
        
    }else if(method == "LASSO"){
        lasso_model <- cv.glmnet(X[tra.idx, ],y[tra.idx, ],
                           alpha = 1,
                           nfolds =10)
        
        lasso_best <- glmnet(X[tra.idx,], y[tra.idx,], alpha = 1,
                             lambda = lasso_model$lambda.min)
        
        return(predict.glmnet(lasso_best, X[-tra.idx,]) - y[-tra.idx,])
    }
}

drawn <- function(...){
    X <<- matrix(rnorm(n * p), nrow = n, ncol = p)
    X <<- cbind(1, X)
    u <<- rnorm(n)
    y_dgp1 <<- X[,1:3] %*% beta[1:3]+u
    y_dgp2 <<- X[,1:21] %*% beta + u
    X <<- X[,2:51]
}

draw_and_get <- function(method, beta, model, dgp){
    drawn()
    get_coef(method, beta, model, dgp)
}

draw_and_pred_minus_test <- function(method, model, dgp){
    drawn()
    get_pred_minus_test(method, model, dgp)
}
```